

- orchestrator to call all the functions of all the stages

- params file, single source of truth for all the params, paths, etc, orchestrator should import params from here itself
(some important ones:: root folder path , trajectory classification model path, firefly patch classification model path,
root folder contains 1 folder initially: 'original videos', which will contain any number of input original videos to process)

- stage0 cleanup: i want the cleanup process automated, basically on each run, run stage0, which deletes every folder except the
original videos folder

- stage1: detect trajectories::  crop and keep all the trajectory images, where you can see the trajectory itself, in a folder, basically first
create an image (and also save it) containing all the trajectories in every 500 frames, so like that for each 500 frames
Then run connected components over each of those images, to get trajectory candidate ids, and save them all to a csv


- stage2: Trajectory_classifier: deep learning model to classify trajectory crops directly, it should save a csv of the selected trajectory ids, get the original candidate trajectory ids, from
the previuous stage csv and then get the corresponding crops and work on them one by one, the final csv this stage generates
should just contains the selected trajectory ids
model is resnet18, binary classifier, between firefly_trajectory and not_firefly_trajectory
also save all the trajectory images to a folder inside the stage2 folder, so that i can also see them
(do this for both positives and negatives)

- stage3: firefly patch classifier::   should go through the csv from the previous stage and then for each trajectory 
fetch the corresponding crops, and then classify them as firefly flash or not, the selected ones should be put into
a csv with the columns::  x,y,w,h,frame_number
model is resnet18, binary classifier, between firefly_flash and noise
also save all the crops to a folder inside the stage3 folder, so that i can also see them
(do this for both positives and negatives)

- stage4: gaussian_centroid_recentering:: recenter them on gaussian centroid, and save to csv
also save all the crops to a folder inside the stage3 folder, so that i can also see them
each crop should be saved with a saturated red dot at the gaussian centroid coordinate pixel

- stage5: rendering:: render the final video with the selected bounding boxes

- each stage must have its own folder, where it will write its output data

- both those models obv classify images, they work on images basically

- at the end of the pipeline make sure to print the time taken for stages 1 to 4 inclusive, and total them also
- seperately print the time taken for stage 5, as its not exactly part of the detection pipeline, rather just a rendering step

- also have loading bars for each stage as well